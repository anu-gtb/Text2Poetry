{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel,GPT2Tokenizer,TrainingArguments,Trainer,TextDataset,DataCollatorForLanguageModeling\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-2 tokenizer and model\n",
    "model_name='gpt2'\n",
    "tokenizer=GPT2Tokenizer.from_pretrained(model_name)\n",
    "model=GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "data=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows and columns in dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Concept</th>\n",
       "      <th>Poem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>Glass is the best insulator as it has the high...</td>\n",
       "      <td>Glass, the material of choice, you see,For ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reaction between hydrogen and oxygen forms water</td>\n",
       "      <td>In the realm of elements, where atoms roam,Two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>The products of the combustion of hydrocarbons...</td>\n",
       "      <td>In the fiery embrace of hydrocarbon is might, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Decomposition is a type of reaction where a si...</td>\n",
       "      <td>In the realm of science, a process unfolds,Whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>Ammonia molecule is trigonal pyramidal with ni...</td>\n",
       "      <td>In the realm of molecules, where atoms roam,A ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
       "22            22            22          22   \n",
       "0              0             0           0   \n",
       "49            49            49          49   \n",
       "4              4             4           4   \n",
       "54            54            54          54   \n",
       "\n",
       "                                              Concept  \\\n",
       "22  Glass is the best insulator as it has the high...   \n",
       "0    Reaction between hydrogen and oxygen forms water   \n",
       "49  The products of the combustion of hydrocarbons...   \n",
       "4   Decomposition is a type of reaction where a si...   \n",
       "54  Ammonia molecule is trigonal pyramidal with ni...   \n",
       "\n",
       "                                                 Poem  \n",
       "22  Glass, the material of choice, you see,For ins...  \n",
       "0   In the realm of elements, where atoms roam,Two...  \n",
       "49  In the fiery embrace of hydrocarbon is might, ...  \n",
       "4   In the realm of science, a process unfolds,Whe...  \n",
       "54  In the realm of molecules, where atoms roam,A ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Five random observations from dataset\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unnecessary column\n",
    "data=data.drop(columns=['Unnamed: 0','Unnamed: 0.2','Unnamed: 0.1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70 entries, 0 to 69\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Concept  70 non-null     object\n",
      " 1   Poem     70 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Dataset Information\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check duplicated values in dataset\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Helper Functions for GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "def load_dataset(file_path,tokenizer,block_size=1024):\n",
    "    train_data=TextDataset(tokenizer=tokenizer,file_path=file_path,block_size=block_size)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data collator\n",
    "def load_data_collator(tokenizer,mlm=False):\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=mlm)\n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for training the model\n",
    "def train(train_file_path,model_name,output_dir,overwrite_output_dir,train_batch_size,epochs):\n",
    "    tokenizer=GPT2Tokenizer.from_pretrained(model_name)\n",
    "    train_data=load_dataset(train_file_path,tokenizer)\n",
    "    \n",
    "    data_collator=load_data_collator(tokenizer)\n",
    "    \n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    model=GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    model.save_pretrained(output_dir)\n",
    "    \n",
    "    training_arguments=TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=train_batch_size,\n",
    "        logging_dir='./log',\n",
    "        logging_steps=25,\n",
    "        save_steps=150,\n",
    "        logging_first_step=True,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    trainer=Trainer(\n",
    "        model=model,\n",
    "        args=training_arguments,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_data\n",
    "    )\n",
    "    hist=trainer.train()\n",
    "    trainer.save_model()\n",
    "    return trainer,hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all the training arguments\n",
    "train_file_path='Text2Poetry/data.csv',
    "model_name=model_name\n",
    "output_dir='./model'\n",
    "overwrite_output_dir=True\n",
    "train_batch_size=3\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbe60178c00449ba7bfc3620aed2d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6096, 'grad_norm': 7.481527805328369, 'learning_rate': 0.000995, 'epoch': 0.5}\n",
      "{'loss': 2.1459, 'grad_norm': 2.437744617462158, 'learning_rate': 0.000875, 'epoch': 12.5}\n",
      "{'loss': 0.1221, 'grad_norm': 1.0298062562942505, 'learning_rate': 0.00075, 'epoch': 25.0}\n",
      "{'loss': 0.0505, 'grad_norm': 0.3540869951248169, 'learning_rate': 0.000625, 'epoch': 37.5}\n",
      "{'loss': 0.02, 'grad_norm': 0.2944098711013794, 'learning_rate': 0.0005, 'epoch': 50.0}\n",
      "{'loss': 0.0083, 'grad_norm': 0.10800329595804214, 'learning_rate': 0.000375, 'epoch': 62.5}\n",
      "{'loss': 0.0044, 'grad_norm': 0.03888916224241257, 'learning_rate': 0.00025, 'epoch': 75.0}\n",
      "{'loss': 0.0022, 'grad_norm': 0.08134657144546509, 'learning_rate': 0.000125, 'epoch': 87.5}\n",
      "{'loss': 0.0017, 'grad_norm': 0.13632521033287048, 'learning_rate': 0.0, 'epoch': 100.0}\n",
      "{'train_runtime': 3281.885, 'train_samples_per_second': 0.152, 'train_steps_per_second': 0.061, 'train_loss': 0.3017345761694014, 'epoch': 100.0}\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "hist=train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    train_batch_size=train_batch_size,\n",
    "    epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
